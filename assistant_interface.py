import os
from openai import OpenAI 
import streamlit as st
import logging
from ratelimit import limits

def create_assistant(client: OpenAI, instructions, model="gpt-4o", tools=None):
    """returns new assistant created with specified model. default model is gpt-4o
    client: OpenAI client object
    instructions: specific direction for assistant (str)
    model: str, default gpt-4o
    tools: [], default=None, functions to be called when required by thread"""
    try:
        assistant = client.beta.assistants.create(model=model, instructions=instructions, tools=tools)
        return assistant
    except Exception as e:
        logging.error(f"Error creating assistant: {e}")
        st.error(f"Sorry it seems there was an error: {e}")
        return None
    
def retrieve_assistant(client: OpenAI, assistant_retrieval_id):
    """
    returns assistant object
    client: OpenAI object
    assistant_retrival_id: str, id of assistant from openai
    """
    try:
        assistant = client.beta.assistants.retrieve(assistant_id=assistant_retrieval_id)
        return assistant
    except Exception as e:
        logging.error(f"Error retrieving assistant: {e}")
        st.error(f"Sorry it seems there was an error: {e}")
        return None

def create_vector_store(client, file_paths):
    """
    client: OpenAI object
    file_paths:[] of files to add to new vector store
    """
    try:
        vector_store = client.beta.vector_stores.create(name="Eldin Ring Info")
        file_streams = [open(path, "rb") for path in file_paths]

        file_batch = client.beta.vector_stores.file_batches.upload_and_poll(
            vector_store_id=vector_store.id, files=file_streams
        )

        logging.info(f"File batch status: {file_batch.status}")
        logging.info(f"File counts: {file_batch.file_counts}")
        return vector_store
    except Exception as e:
        logging.error(f"Error creating vector store: {e}")
        st.error(f"Sorry it seems there was an error: {e}")
        return None

def attach_vector_store(client, assistant, vector_store_id):
    """
    returns assistant with vector store attached
    client: OpenAI api
    assistant: assistant object
    vector_store_id: id of vector store
    """
    try:
        assistant = client.beta.assistants.update(
            assistant_id=assistant.id,
            tool_resources={"file_search": {"vector_store_ids": [vector_store_id]}},
        )
        return assistant
    except Exception as e:
        logging.error(f"Error attaching vector store: {e}")
        st.error(f"Sorry it seems there was an error: {e}")
        return None

def create_thread(client):
  """
  returns thread
  client: OpenAI object
  """
  try:
    thread = client.beta.threads.create()
    return thread
  except Exception as e:
    logging.error(f"Error creating thread: {e}")
    st.error(f"Sorry it seems there was an error: {e}. Please reload the page")
    return None

@limits(calls=20, period=60) #20 calls per minute
def add_message_to_thread(thread, content, client):
  """
  returns: message object
  thread: thread object
  content: str, message to add to thread
  client: OpenAI object
  """
  try:
    message = client.beta.threads.messages.create(
      thread_id=thread.id,
      role="user",
      content=content
    )
    return message
  except Exception as e:
    logging.error(f"Error adding message to thread: {e}")
    st.error(f"Sorry it seems there was an error: {e}")
    return None
  
@limits(calls=10, period=60) #10 calls per minute
def get_assitant_messages(client, thread, assistant, function=None):
  """
  returns: str, message response from assistant. manages state of request 
  and handles call to external functions as needed by assistant
  client: OpenAI object
  thread: thread object
  assistant: assistant object
  """
  try:
    run = client.beta.threads.runs.create_and_poll(
      thread_id=thread.id, assistant_id=assistant.id, 
    )
    if run.status == "requires_action":

      #GET QUERY GENERATED BY ASSISTANT
      query = str(run.required_action.submit_tool_outputs.tool_calls[0].function.arguments)
      tool_outputs = []

      #ITERATE OVER ALL TOOL CALLS AND INVOKE FUNCTION CALL
      for tool in run.required_action.submit_tool_outputs.tool_calls:
        if tool.function.name == "get_research":
          tool_outputs.append({
            "tool_call_id": tool.id,
            "output": function(query)
          })
      
      if tool_outputs:
        #SEND TOOL OUTPUTS
        try:
          run = client.beta.threads.runs.submit_tool_outputs_and_poll(
            thread_id=thread.id,
            run_id=run.id,
            tool_outputs=tool_outputs
          )
          print("Tool outputs submitted successfully.")
        except Exception as e:
            print("Failed to submit tool outputs:", e)
        else:
          print("No tool outputs to submit.")
    elif run.status == "failed":
        print("failed")
        raise Exception
          
    if run.status == "completed":

      #GET MESSAGE TEXT
      messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))
      message_content = messages[0].content[0].text
      return message_content.value
      
    
  except Exception as e:
    logging.error(f"Error sending messages: {e}")
    st.error(f"Sorry it seems there was an error: {e}")
    return None